RAG_LangChain Project: Web Question-Answering

Description:::::

This project allows users to input a webpage URL and ask questions about its content. The system uses Retrieval-Augmented Generation (RAG) with LangChain and a local LLaMA 3 model via Ollama to answer questions intelligently.

Project Architecture :::: 

1...... Load URL

The webpage is loaded using a WebBaseLoader, extracting the textual content.

2...... Split into Chunks

The text is split into manageable pieces using RecursiveCharacterTextSplitter.

This ensures embeddings capture context efficiently.

3...... Generate Embeddings

Each chunk is converted into a vector using Ollama embeddings (LLaMA 3).

4...... Vector Store

Embeddings are stored in a Chroma vector database for fast similarity search.

5...... Call LLM (Ollama = LLaMA 3)

When a question is asked, the LLM uses the retrieved context to generate a natural language answer.

6...... Retrieve & Answer

The user’s query is sent to the retriever, which finds the most relevant chunks.

The LLM then generates a response based on this retrieved information.



Flow Diagram (Textual)::::: 


User Input: Web URL
        ↓
Load Webpage Content
        ↓
Split Text into Chunks
        ↓
Generate Embeddings (Ollama LLaMA3)
        ↓
Store in Vector Store (Chroma)
        ↓
User Question
        ↓
Retrieve Relevant Chunks
        ↓
LLM Generates Answer
        ↓
Output Answer to User






Key Features::::::

Ask any question about a webpage

Uses local LLaMA 3 model (Ollama) → offline, fast

Retrieval ensures answers are grounded in the webpage

Modular: can extend to multiple webpages or other sources